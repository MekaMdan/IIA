{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Trabalho_4_IIA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX3RZxf4jIce"
      },
      "source": [
        "Fernanda Macedo de Sousa - 17/0010058\n",
        "\n",
        "Mariana Alencar do Vale - 16/0014522  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tEdps9bcseA"
      },
      "source": [
        "O projeto visa usar e testar variações de uma rede CNN (Rede Neuronal Convolucional, ou *Convolutional Neural Network*) implementada no *framework* Keras para reconhecimento de dígitos manuscritos, a partir da base do MNIST.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAbA8OYFde-6"
      },
      "source": [
        "# Importando o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEVlz24kc1jq"
      },
      "source": [
        "\n",
        "from keras.datasets.mnist import load_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw5WE5QOe0M9"
      },
      "source": [
        "Carregando os dígitos de treino e os dígitos de teste, em formato de tuplas com seus respectivos rótulos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beUqv1NCexav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd3cd8a-00f8-42b8-d592-23cf90eab757"
      },
      "source": [
        "(train_digits, train_labels), (test_digits, test_labels) = load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edT4mmmfkb1P"
      },
      "source": [
        "# Pré processamento dos dados do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baD7Vvn9kjqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3e27c6e-dbf4-4eb9-db91-967694ec76b3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "image_height = train_digits.shape[1]  \n",
        "image_width = train_digits.shape[2]\n",
        "num_channels = 1  \n",
        "\n",
        "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
        "test_data = np.reshape(test_digits, (test_digits.shape[0],image_height, image_width, num_channels))\n",
        "\n",
        "train_data = train_data.astype('float32') / 255.\n",
        "test_data = test_data.astype('float32') / 255.\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "train_labels_cat = to_categorical(train_labels,num_classes)\n",
        "test_labels_cat = to_categorical(test_labels,num_classes)\n",
        "train_labels_cat.shape, test_labels_cat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph3fVKfKnDt9"
      },
      "source": [
        "Os dados foram normalizados. O Dataset foi dividido em 60.000 imagens de treino e 10.000 imagens de teste. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxQEgg35mTsp"
      },
      "source": [
        "# Separação de uma parte dos dados de treino do Dataset para validação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAkeZI0umbUC"
      },
      "source": [
        "for _ in range(5): \n",
        "    indexes = np.random.permutation(len(train_data))\n",
        "\n",
        "train_data = train_data[indexes]\n",
        "train_labels_cat = train_labels_cat[indexes]\n",
        "\n",
        "val_perc = 0.10\n",
        "val_count = int(val_perc * len(train_data))\n",
        "\n",
        "val_data = train_data[:val_count,:]\n",
        "val_labels_cat = train_labels_cat[:val_count,:]\n",
        "\n",
        "train_data2 = train_data[val_count:,:]\n",
        "train_labels_cat2 = train_labels_cat[val_count:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fodhjRpah_"
      },
      "source": [
        "Os dados foram embaralhados para garantir que não existe um padrão. Então foi definido que os primeiros 10% dos dados serão utilizados para a validação do modelo. Os outros 90% serão utilizados para treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpArLvinqEgM"
      },
      "source": [
        "# Construção da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHqmZYNWqMSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00452d9d-2ba6-4b9e-e882-7aba552c2a8d"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # add Convolutional layers\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
        "                     input_shape=(image_height, image_width, num_channels)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))    \n",
        "    model.add(Flatten())\n",
        "    # Densely connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # output layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # compile with adam optimizer & categorical_crossentropy loss function\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 130,890\n",
            "Trainable params: 130,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3V2YJ5tUCW"
      },
      "source": [
        "Inicialmente, nosso modelo CNN consiste em 3 camadas convolucionais, com uma camada *MaxPooling2D* com *pool_size=(2,2)* imediatamente após cada uma delas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4EcuCQyuUte"
      },
      "source": [
        "# Treino da CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3RF27qKua2o",
        "outputId": "cf48a005-7007-474e-8469-b7c306e6d854"
      },
      "source": [
        "results = model.fit(train_data2, train_labels_cat2, \n",
        "                    epochs=15, batch_size=64,\n",
        "                    validation_data=(val_data, val_labels_cat))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "844/844 [==============================] - 74s 87ms/step - loss: 0.1862 - accuracy: 0.9412 - val_loss: 0.0595 - val_accuracy: 0.9815\n",
            "Epoch 2/15\n",
            "844/844 [==============================] - 74s 87ms/step - loss: 0.0489 - accuracy: 0.9851 - val_loss: 0.0409 - val_accuracy: 0.9878\n",
            "Epoch 3/15\n",
            "844/844 [==============================] - 74s 87ms/step - loss: 0.0355 - accuracy: 0.9886 - val_loss: 0.0410 - val_accuracy: 0.9877\n",
            "Epoch 4/15\n",
            "844/844 [==============================] - 74s 87ms/step - loss: 0.0262 - accuracy: 0.9917 - val_loss: 0.0401 - val_accuracy: 0.9878\n",
            "Epoch 5/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0345 - val_accuracy: 0.9905\n",
            "Epoch 6/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.0430 - val_accuracy: 0.9863\n",
            "Epoch 7/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 0.0428 - val_accuracy: 0.9888\n",
            "Epoch 8/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.0339 - val_accuracy: 0.9902\n",
            "Epoch 9/15\n",
            "844/844 [==============================] - 74s 87ms/step - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.0378 - val_accuracy: 0.9893\n",
            "Epoch 10/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0106 - accuracy: 0.9965 - val_loss: 0.0362 - val_accuracy: 0.9893\n",
            "Epoch 11/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0311 - val_accuracy: 0.9915\n",
            "Epoch 12/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.0083 - accuracy: 0.9971 - val_loss: 0.0409 - val_accuracy: 0.9885\n",
            "Epoch 13/15\n",
            "844/844 [==============================] - 74s 87ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.0389 - val_accuracy: 0.9898\n",
            "Epoch 14/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0395 - val_accuracy: 0.9910\n",
            "Epoch 15/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0374 - val_accuracy: 0.9902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0biP0Ev0_eT"
      },
      "source": [
        "# Avaliação de resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VZ1P6o_2kRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635a28f1-e3be-49c5-c7a9-2137cd765bf1"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels_cat, batch_size=64)\n",
        "print('Test loss: %.4f accuracy: %.4f' % (test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 4s 25ms/step - loss: 0.0345 - accuracy: 0.9925\n",
            "Test loss: 0.0345 accuracy: 0.9925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_OIyhsm3kUR"
      },
      "source": [
        "A CNN com 3 camadas convolucionais, dados normalizados, com *MaxPooling* após cada camada convolucional, usando 10% dos dados de treinamento para validação e 90% restantes para treinamento, obteve 99.1% de acurácia. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z82umyGu53QO"
      },
      "source": [
        "# Avaliação de resultados da CNN para esse Dataset sem normalizar os dados "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fotaHdQ_76zo"
      },
      "source": [
        "Analogamente, seguimos os passos de importação do Dataset em tuplas de treino e teste, pré processamento dos dados (embora sem a normalização) e separamos os dados de treino com 10% para validação e 90% para treino, assim como o modelo anterior. Da mesma forma, construimos a CNN e treinamos, a fim de avaliar a diferença dos resultados com e sem uma normalização dos dados. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5IVjJMR6Ko3"
      },
      "source": [
        "(train_digits, train_labels), (test_digits, test_labels) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhUHmHvI6QsD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1603838d-b297-4356-f6cd-c8dcc869bfd5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "image_height = train_digits.shape[1]  \n",
        "image_width = train_digits.shape[2]\n",
        "num_channels = 1  \n",
        "\n",
        "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
        "test_data = np.reshape(test_digits, (test_digits.shape[0],image_height, image_width, num_channels))\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "train_labels_cat = to_categorical(train_labels,num_classes)\n",
        "test_labels_cat = to_categorical(test_labels,num_classes)\n",
        "train_labels_cat.shape, test_labels_cat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uDqOfI76gGr"
      },
      "source": [
        "for _ in range(5): \n",
        "    indexes = np.random.permutation(len(train_data))\n",
        "\n",
        "train_data = train_data[indexes]\n",
        "train_labels_cat = train_labels_cat[indexes]\n",
        "\n",
        "val_perc = 0.10\n",
        "val_count = int(val_perc * len(train_data))\n",
        "\n",
        "val_data = train_data[:val_count,:]\n",
        "val_labels_cat = train_labels_cat[:val_count,:]\n",
        "\n",
        "train_data2 = train_data[val_count:,:]\n",
        "train_labels_cat2 = train_labels_cat[val_count:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfwJ9-wx6ro8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94b93ec-adda-402e-97d0-772f43975c91"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # add Convolutional layers\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
        "                     input_shape=(image_height, image_width, num_channels)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))    \n",
        "    model.add(Flatten())\n",
        "    # Densely connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # output layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # compile with adam optimizer & categorical_crossentropy loss function\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 130,890\n",
            "Trainable params: 130,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ky085a61Fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c52b9673-4160-4611-ecce-9cd686b773c4"
      },
      "source": [
        "results = model.fit(train_data2, train_labels_cat2, \n",
        "                    epochs=15, batch_size=64,\n",
        "                    validation_data=(val_data, val_labels_cat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.3718 - accuracy: 0.9306 - val_loss: 0.0806 - val_accuracy: 0.9753\n",
            "Epoch 2/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0629 - accuracy: 0.9809 - val_loss: 0.0583 - val_accuracy: 0.9818\n",
            "Epoch 3/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.0462 - accuracy: 0.9851 - val_loss: 0.0594 - val_accuracy: 0.9835\n",
            "Epoch 4/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0405 - accuracy: 0.9871 - val_loss: 0.0524 - val_accuracy: 0.9860\n",
            "Epoch 5/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0332 - accuracy: 0.9894 - val_loss: 0.0499 - val_accuracy: 0.9843\n",
            "Epoch 6/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0329 - accuracy: 0.9898 - val_loss: 0.0683 - val_accuracy: 0.9823\n",
            "Epoch 7/15\n",
            "844/844 [==============================] - 74s 88ms/step - loss: 0.0265 - accuracy: 0.9914 - val_loss: 0.0554 - val_accuracy: 0.9860\n",
            "Epoch 8/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0277 - accuracy: 0.9912 - val_loss: 0.0452 - val_accuracy: 0.9870\n",
            "Epoch 9/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.0224 - accuracy: 0.9931 - val_loss: 0.0566 - val_accuracy: 0.9842\n",
            "Epoch 10/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.0188 - accuracy: 0.9941 - val_loss: 0.0538 - val_accuracy: 0.9877\n",
            "Epoch 11/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0202 - accuracy: 0.9940 - val_loss: 0.0537 - val_accuracy: 0.9887\n",
            "Epoch 12/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0179 - accuracy: 0.9945 - val_loss: 0.0530 - val_accuracy: 0.9898\n",
            "Epoch 13/15\n",
            "844/844 [==============================] - 73s 87ms/step - loss: 0.0208 - accuracy: 0.9935 - val_loss: 0.0647 - val_accuracy: 0.9835\n",
            "Epoch 14/15\n",
            "844/844 [==============================] - 73s 86ms/step - loss: 0.0155 - accuracy: 0.9953 - val_loss: 0.0586 - val_accuracy: 0.9887\n",
            "Epoch 15/15\n",
            "844/844 [==============================] - 75s 88ms/step - loss: 0.0139 - accuracy: 0.9960 - val_loss: 0.0535 - val_accuracy: 0.9887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jtBJItG_VPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028ca98c-859b-491e-a2ff-adec191783cb"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels_cat, batch_size=64)\n",
        "print('Test loss: %.4f accuracy: %.4f' % (test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 4s 25ms/step - loss: 0.0448 - accuracy: 0.9902\n",
            "Test loss: 0.0448 accuracy: 0.9902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhzZUgWb_8hi"
      },
      "source": [
        "Ao repetir os mesmos passos com os dados não normalizados, obtivemos uma acurácia de 98.46%, ou seja, 0.54% menor do que o modelo CNN que utiliza os dados normalizados (implementado anteriormente). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAioAuI5QoUK"
      },
      "source": [
        "# Usando somente uma camada convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GZhEyAwdBKC"
      },
      "source": [
        "Nesta etapa, deixaremos apenas uma camada convolucional e avaliaremos o desempenho. Será utilizado 90% de dados para treinamento e 10% validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc16P3yMQnzm",
        "outputId": "4dc30565-05a6-41fa-bf8b-4124c9861e78"
      },
      "source": [
        "(train_digits, train_labels), (test_digits, test_labels) = load_data()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "image_height = train_digits.shape[1]  \n",
        "image_width = train_digits.shape[2]\n",
        "num_channels = 1  \n",
        "\n",
        "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
        "test_data = np.reshape(test_digits, (test_digits.shape[0],image_height, image_width, num_channels))\n",
        "\n",
        "train_data = train_data.astype('float32') / 255.\n",
        "test_data = test_data.astype('float32') / 255.\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "train_labels_cat = to_categorical(train_labels,num_classes)\n",
        "test_labels_cat = to_categorical(test_labels,num_classes)\n",
        "train_labels_cat.shape, test_labels_cat.shape\n",
        "\n",
        "for _ in range(5): \n",
        "    indexes = np.random.permutation(len(train_data))\n",
        "\n",
        "train_data = train_data[indexes]\n",
        "train_labels_cat = train_labels_cat[indexes]\n",
        "\n",
        "val_perc = 0.1\n",
        "val_count = int(val_perc * len(train_data))\n",
        "\n",
        "val_data = train_data[:val_count,:]\n",
        "val_labels_cat = train_labels_cat[:val_count,:]\n",
        "\n",
        "train_data2 = train_data[val_count:,:]\n",
        "train_labels_cat2 = train_labels_cat[val_count:,:]\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # add Convolutional layers\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
        "                     input_shape=(image_height, image_width, num_channels)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2))) \n",
        "    model.add(Flatten())\n",
        "    # Densely connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # output layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # compile with adam optimizer & categorical_crossentropy loss function\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "print(model.summary())\n",
        "\n",
        "results = model.fit(train_data2, train_labels_cat2, \n",
        "                    epochs=15, batch_size=64,\n",
        "                    validation_data=(val_data, val_labels_cat))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels_cat, batch_size=64)\n",
        "print('Test loss: %.4f accuracy: %.4f' % (test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 804,554\n",
            "Trainable params: 804,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "844/844 [==============================] - 28s 33ms/step - loss: 0.1931 - accuracy: 0.9441 - val_loss: 0.0837 - val_accuracy: 0.9765\n",
            "Epoch 2/15\n",
            "844/844 [==============================] - 29s 34ms/step - loss: 0.0642 - accuracy: 0.9804 - val_loss: 0.0765 - val_accuracy: 0.9768\n",
            "Epoch 3/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 0.0695 - val_accuracy: 0.9775\n",
            "Epoch 4/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0302 - accuracy: 0.9906 - val_loss: 0.0626 - val_accuracy: 0.9802\n",
            "Epoch 5/15\n",
            "844/844 [==============================] - 29s 34ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.0675 - val_accuracy: 0.9810\n",
            "Epoch 6/15\n",
            "844/844 [==============================] - 29s 35ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.0509 - val_accuracy: 0.9847\n",
            "Epoch 7/15\n",
            "844/844 [==============================] - 29s 34ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0539 - val_accuracy: 0.9847\n",
            "Epoch 8/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.0520 - val_accuracy: 0.9867\n",
            "Epoch 9/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 0.0631 - val_accuracy: 0.9840\n",
            "Epoch 10/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0695 - val_accuracy: 0.9843\n",
            "Epoch 11/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0057 - accuracy: 0.9982 - val_loss: 0.0616 - val_accuracy: 0.9848\n",
            "Epoch 12/15\n",
            "844/844 [==============================] - 28s 34ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0611 - val_accuracy: 0.9868\n",
            "Epoch 13/15\n",
            "844/844 [==============================] - 28s 33ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0720 - val_accuracy: 0.9830\n",
            "Epoch 14/15\n",
            "844/844 [==============================] - 29s 34ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0760 - val_accuracy: 0.9848\n",
            "Epoch 15/15\n",
            "844/844 [==============================] - 29s 34ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0715 - val_accuracy: 0.9842\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.0683 - accuracy: 0.9850\n",
            "Test loss: 0.0683 accuracy: 0.9850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvqcBK-FQyQb"
      },
      "source": [
        "# Usando duas camadas convolucionais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFkCvcuuc3fw"
      },
      "source": [
        "Nesta etapa, iremos refazer todo o experimento, devolvendo uma das camadas convolucionais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLYvr1mtVBu_",
        "outputId": "8af1bb0d-4b7b-404a-8c1b-a83cb7ff27de"
      },
      "source": [
        "(train_digits, train_labels), (test_digits, test_labels) = load_data()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "image_height = train_digits.shape[1]  \n",
        "image_width = train_digits.shape[2]\n",
        "num_channels = 1  \n",
        "\n",
        "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
        "test_data = np.reshape(test_digits, (test_digits.shape[0],image_height, image_width, num_channels))\n",
        "\n",
        "train_data = train_data.astype('float32') / 255.\n",
        "test_data = test_data.astype('float32') / 255.\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "train_labels_cat = to_categorical(train_labels,num_classes)\n",
        "test_labels_cat = to_categorical(test_labels,num_classes)\n",
        "train_labels_cat.shape, test_labels_cat.shape\n",
        "\n",
        "for _ in range(5): \n",
        "    indexes = np.random.permutation(len(train_data))\n",
        "\n",
        "train_data = train_data[indexes]\n",
        "train_labels_cat = train_labels_cat[indexes]\n",
        "\n",
        "val_perc = 0.1\n",
        "val_count = int(val_perc * len(train_data))\n",
        "\n",
        "val_data = train_data[:val_count,:]\n",
        "val_labels_cat = train_labels_cat[:val_count,:]\n",
        "\n",
        "train_data2 = train_data[val_count:,:]\n",
        "train_labels_cat2 = train_labels_cat[val_count:,:]\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # add Convolutional layers\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
        "                     input_shape=(image_height, image_width, num_channels)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2))) \n",
        "    model.add(Flatten())\n",
        "    # Densely connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # output layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # compile with adam optimizer & categorical_crossentropy loss function\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "print(model.summary())\n",
        "\n",
        "results = model.fit(train_data2, train_labels_cat2, \n",
        "                    epochs=15, batch_size=64,\n",
        "                    validation_data=(val_data, val_labels_cat))\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels_cat, batch_size=64)\n",
        "print('Test loss: %.4f accuracy: %.4f' % (test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               401536    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 421,642\n",
            "Trainable params: 421,642\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "844/844 [==============================] - 60s 71ms/step - loss: 0.1566 - accuracy: 0.9527 - val_loss: 0.0554 - val_accuracy: 0.9828\n",
            "Epoch 2/15\n",
            "844/844 [==============================] - 60s 71ms/step - loss: 0.0447 - accuracy: 0.9856 - val_loss: 0.0382 - val_accuracy: 0.9882\n",
            "Epoch 3/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0297 - accuracy: 0.9906 - val_loss: 0.0379 - val_accuracy: 0.9895\n",
            "Epoch 4/15\n",
            "844/844 [==============================] - 60s 71ms/step - loss: 0.0228 - accuracy: 0.9926 - val_loss: 0.0308 - val_accuracy: 0.9912\n",
            "Epoch 5/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0170 - accuracy: 0.9943 - val_loss: 0.0306 - val_accuracy: 0.9917\n",
            "Epoch 6/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0344 - val_accuracy: 0.9918\n",
            "Epoch 7/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.0364 - val_accuracy: 0.9900\n",
            "Epoch 8/15\n",
            "844/844 [==============================] - 60s 71ms/step - loss: 0.0086 - accuracy: 0.9970 - val_loss: 0.0473 - val_accuracy: 0.9877\n",
            "Epoch 9/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0493 - val_accuracy: 0.9878\n",
            "Epoch 10/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0054 - accuracy: 0.9983 - val_loss: 0.0518 - val_accuracy: 0.9895\n",
            "Epoch 11/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0056 - accuracy: 0.9980 - val_loss: 0.0428 - val_accuracy: 0.9913\n",
            "Epoch 12/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0500 - val_accuracy: 0.9900\n",
            "Epoch 13/15\n",
            "844/844 [==============================] - 60s 71ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0428 - val_accuracy: 0.9917\n",
            "Epoch 14/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0599 - val_accuracy: 0.9882\n",
            "Epoch 15/15\n",
            "844/844 [==============================] - 59s 70ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0476 - val_accuracy: 0.9905\n",
            "157/157 [==============================] - 3s 21ms/step - loss: 0.0372 - accuracy: 0.9917\n",
            "Test loss: 0.0372 accuracy: 0.9917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpOp8rkXQWkz"
      },
      "source": [
        "# Para 30% validação e 70% treinamento\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96R1pULocXeD"
      },
      "source": [
        "Nesta etapa, foi refeito todo o processo (normalizado) mudando apenas a porcentagem de treinamento e validação com três camadas convolucionais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_JJgk6RL0e"
      },
      "source": [
        "(train_digits, train_labels), (test_digits, test_labels) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fhgtpxeRNaM",
        "outputId": "02a84176-dc64-436e-fb1a-80119622f35a"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "image_height = train_digits.shape[1]  \n",
        "image_width = train_digits.shape[2]\n",
        "num_channels = 1  \n",
        "\n",
        "train_data = np.reshape(train_digits, (train_digits.shape[0], image_height, image_width, num_channels))\n",
        "test_data = np.reshape(test_digits, (test_digits.shape[0],image_height, image_width, num_channels))\n",
        "\n",
        "train_data = train_data.astype('float32') / 255.\n",
        "test_data = test_data.astype('float32') / 255.\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "num_classes = 10\n",
        "train_labels_cat = to_categorical(train_labels,num_classes)\n",
        "test_labels_cat = to_categorical(test_labels,num_classes)\n",
        "train_labels_cat.shape, test_labels_cat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 10), (10000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z6wF_LJRRJg"
      },
      "source": [
        "for _ in range(5): \n",
        "    indexes = np.random.permutation(len(train_data))\n",
        "\n",
        "train_data = train_data[indexes]\n",
        "train_labels_cat = train_labels_cat[indexes]\n",
        "\n",
        "val_perc = 0.3\n",
        "val_count = int(val_perc * len(train_data))\n",
        "\n",
        "val_data = train_data[:val_count,:]\n",
        "val_labels_cat = train_labels_cat[:val_count,:]\n",
        "\n",
        "train_data2 = train_data[val_count:,:]\n",
        "train_labels_cat2 = train_labels_cat[val_count:,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8g8syG1RZ6F",
        "outputId": "4793d159-e8e8-4eb9-8777-960377b7214f"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    # add Convolutional layers\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same',\n",
        "                     input_shape=(image_height, image_width, num_channels)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))    \n",
        "    model.add(Flatten())\n",
        "    # Densely connected layers\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    # output layer\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    # compile with adam optimizer & categorical_crossentropy loss function\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 14, 14, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 7, 7, 64)          36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 128)               73856     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 130,890\n",
            "Trainable params: 130,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGyNjt-iRa2D",
        "outputId": "67ca0fd5-1347-45ad-cc0b-e1204dce57bd"
      },
      "source": [
        "results = model.fit(train_data2, train_labels_cat2, \n",
        "                    epochs=15, batch_size=64,\n",
        "                    validation_data=(val_data, val_labels_cat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "657/657 [==============================] - 63s 96ms/step - loss: 0.2117 - accuracy: 0.9361 - val_loss: 0.0591 - val_accuracy: 0.9822\n",
            "Epoch 2/15\n",
            "657/657 [==============================] - 63s 96ms/step - loss: 0.0550 - accuracy: 0.9831 - val_loss: 0.0783 - val_accuracy: 0.9758\n",
            "Epoch 3/15\n",
            "657/657 [==============================] - 64s 97ms/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 0.0418 - val_accuracy: 0.9862\n",
            "Epoch 4/15\n",
            "657/657 [==============================] - 63s 96ms/step - loss: 0.0317 - accuracy: 0.9895 - val_loss: 0.0362 - val_accuracy: 0.9883\n",
            "Epoch 5/15\n",
            "657/657 [==============================] - 63s 96ms/step - loss: 0.0250 - accuracy: 0.9919 - val_loss: 0.0425 - val_accuracy: 0.9869\n",
            "Epoch 6/15\n",
            "657/657 [==============================] - 64s 98ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0357 - val_accuracy: 0.9893\n",
            "Epoch 7/15\n",
            "657/657 [==============================] - 64s 98ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.0364 - val_accuracy: 0.9898\n",
            "Epoch 8/15\n",
            "657/657 [==============================] - 62s 95ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0312 - val_accuracy: 0.9912\n",
            "Epoch 9/15\n",
            "657/657 [==============================] - 62s 94ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 0.0318 - val_accuracy: 0.9913\n",
            "Epoch 10/15\n",
            "657/657 [==============================] - 62s 94ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0408 - val_accuracy: 0.9884\n",
            "Epoch 11/15\n",
            "657/657 [==============================] - 62s 95ms/step - loss: 0.0086 - accuracy: 0.9968 - val_loss: 0.0402 - val_accuracy: 0.9892\n",
            "Epoch 12/15\n",
            "657/657 [==============================] - 63s 96ms/step - loss: 0.0096 - accuracy: 0.9968 - val_loss: 0.0335 - val_accuracy: 0.9913\n",
            "Epoch 13/15\n",
            "657/657 [==============================] - 62s 95ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0329 - val_accuracy: 0.9911\n",
            "Epoch 14/15\n",
            "657/657 [==============================] - 62s 94ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.0342 - val_accuracy: 0.9918\n",
            "Epoch 15/15\n",
            "657/657 [==============================] - 61s 94ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0402 - val_accuracy: 0.9907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i6XXaj4ReTH",
        "outputId": "ac2dd3b0-66cb-458f-8b3d-a8f312830172"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels_cat, batch_size=64)\n",
        "print('Test loss: %.4f accuracy: %.4f' % (test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 4s 25ms/step - loss: 0.0366 - accuracy: 0.9915\n",
            "Test loss: 0.0366 accuracy: 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}